{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: tf-idf Definition\n"
      ],
      "metadata": {
        "id": "x_LZMf3hs3Vu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUSkF6od1P8",
        "outputId": "42ceaf93-ce21-4862-9890-2a3135b8c8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  16.4M      0  0:00:02  0:00:02 --:--:-- 16.5M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.csv(\"/content/agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))\n"
      ],
      "metadata": {
        "id": "t1aAUttQd70t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UusmD1bLvBgI",
        "outputId": "d6e5c5c2-ed58-41c4-e084-03a3e56fe627"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_phase_tf(documents):\n",
        "  \"\"\"\n",
        "  mapping function for tf-idf\n",
        "  takes a list of dictionaries with keys 'id' and 'filtered'\n",
        "  returns a list of tuples with keys (doc_id, word) and values 1\n",
        "  \"\"\"\n",
        "  mapped = []\n",
        "  for row in documents: #loops through each document in the input list\n",
        "        doc_id = row['id'] #finds the document id\n",
        "        words = row['filtered'] #finds the list of words\n",
        "        for word in words:\n",
        "            mapped.append(((doc_id, word), 1))  #for counting word frequency in the document\n",
        "            mapped.append((word, doc_id))       #for counting document frequency\n",
        "  return mapped\n"
      ],
      "metadata": {
        "id": "jdmrS1mT1NYg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def shuffle_and_sort(mapped_data):\n",
        "  \"\"\"\n",
        "  shuffle and sort function for tf-idf\n",
        "  takes a list of tuples with keys (doc_id, word) and values 1\n",
        "  returns a dictionary with keys (doc_id, word) and values a list of counts\n",
        "  and a dictionary with keys word and values a set of doc_ids\n",
        "  \"\"\"\n",
        "  tf_grouped = defaultdict(list) #stores term frequency\n",
        "  df_grouped = defaultdict(set) #stores document frequency\n",
        "\n",
        "  for key, value in mapped_data: #iterating over all key-value pairs from the mapped_data\n",
        "        if isinstance(key, tuple):  #checking if the key is part of the term frequency\n",
        "            tf_grouped[key].append(value)\n",
        "        else:  #otherwise the key is part of the document frequency\n",
        "            df_grouped[key].add(value)\n",
        "\n",
        "  return tf_grouped, df_grouped #returning both term and document frequency results for use in the reduce function\n"
      ],
      "metadata": {
        "id": "kfXTqgeT1P2o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_phase_tf(tf_grouped):\n",
        "    \"\"\"\n",
        "    reduce function for term frequency\n",
        "    takes a dictionary with keys (doc_id, word) and values a list of counts\n",
        "    returns a dictionary with keys (doc_id, word) and values as term frequency values\n",
        "    \"\"\"\n",
        "    tf_dict = {}\n",
        "    total_terms_per_doc = defaultdict(int) #counter for total terms per document\n",
        "\n",
        "    for (doc_id, word), counts in tf_grouped.items(): #counting how many times each term appears per document\n",
        "        word_count = sum(counts)\n",
        "        tf_dict[(doc_id, word)] = word_count\n",
        "        total_terms_per_doc[doc_id] = total_terms_per_doc[doc_id] + word_count #total word counts for this document\n",
        "\n",
        "    tf_values = {}\n",
        "    for (doc_id, word), count in tf_dict.items(): #normalizing the word count by total words in the same document\n",
        "        tf = count / total_terms_per_doc[doc_id] #using the formula given for term frequency\n",
        "        tf_values[(doc_id, word)] = tf #stores the final tf value\n",
        "\n",
        "    return tf_values\n"
      ],
      "metadata": {
        "id": "9uUMf7tR1SqY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def reduce_phase_df(df_grouped, total_docs):\n",
        "  \"\"\"\n",
        "  reduce function for document frequency\n",
        "  takes a dictionary with keys word and values a set of doc_ids\n",
        "  returns a dictionary with keys (word and values) as document frequency values\n",
        "  \"\"\"\n",
        "  idf_values = {}\n",
        "  for word, doc_ids in df_grouped.items(): #loop through each term and its associated document id\n",
        "        df = len(doc_ids) #number of unique documents the word appears in\n",
        "        idf = math.log(total_docs / df) #using the given formula to calculate the inverse document frequency\n",
        "        idf_values[word] = idf #stores the final idf value\n",
        "  return idf_values\n"
      ],
      "metadata": {
        "id": "SLy7FtGR1Vv3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(tf_values, idf_values):\n",
        "  \"\"\"\n",
        "  computes the final tf-idf\n",
        "  takes a dictionary with keys (doc_id, word) and values as term frequency values\n",
        "  and a dictionary with keys word and values as document frequency values\n",
        "  returns a dictionary with keys (doc_id, word) and values as tf-idf values\n",
        "  \"\"\"\n",
        "  tfidf_values = {}\n",
        "  for (doc_id, word), tf in tf_values.items(): #looping through all the key-value pairs\n",
        "        idf = idf_values.get(word, 0) #getting the idf value for the word\n",
        "        tfidf_values[(doc_id, word)] = tf * idf #calculating the final tf-idf score based on the given formula\n",
        "  return tfidf_values\n"
      ],
      "metadata": {
        "id": "Po7g5yH61ZLv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agnews.select(\"_c0\").distinct().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slkic7v7heBv",
        "outputId": "9d1cbd7e-c096-480f-ec06-3ab338e620fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "| _c0|\n",
            "+----+\n",
            "| 148|\n",
            "| 463|\n",
            "| 471|\n",
            "| 496|\n",
            "| 833|\n",
            "|1088|\n",
            "|1238|\n",
            "|1342|\n",
            "|1580|\n",
            "|1591|\n",
            "|1645|\n",
            "|1829|\n",
            "|1959|\n",
            "|2122|\n",
            "|2142|\n",
            "|2366|\n",
            "|2659|\n",
            "|2866|\n",
            "|3175|\n",
            "|3749|\n",
            "+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = agnews.select(\n",
        "    F.col(\"_c0\").alias(\"id\"),\n",
        "    F.col(\"filtered\")\n",
        ").rdd.map(lambda row: row.asDict()).collect() #changing the column name to id so that it is easier to call instead of \"_c0\"\n",
        "\n",
        "# calling all the different functions involved\n",
        "mapped = map_phase_tf(documents)\n",
        "tf_grouped, df_grouped = shuffle_and_sort(mapped)\n",
        "total_docs = len(set(row['id'] for row in documents))\n",
        "tf_values = reduce_phase_tf(tf_grouped)\n",
        "idf_values = reduce_phase_df(df_grouped, total_docs)\n",
        "\n",
        "# computing the final tf-idf values\n",
        "tfidf = compute_tfidf(tf_values, idf_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6t3RZhdfXk0",
        "outputId": "acf8c7d8-8f65-4dfb-eabf-0e4a664e6738"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc: 0, Word: 'wall', TF-IDF: 0.5116\n",
            "Doc: 0, Word: 'st', TF-IDF: 0.2585\n",
            "Doc: 0, Word: 'bears', TF-IDF: 0.3372\n",
            "Doc: 0, Word: 'claw', TF-IDF: 0.4991\n",
            "Doc: 0, Word: 'back', TF-IDF: 0.1892\n",
            "Doc: 0, Word: 'black', TF-IDF: 0.2953\n",
            "Doc: 0, Word: 'reuters', TF-IDF: 0.2475\n",
            "Doc: 0, Word: 'short', TF-IDF: 0.2773\n",
            "Doc: 0, Word: 'sellers', TF-IDF: 0.4468\n",
            "Doc: 0, Word: 'street', TF-IDF: 0.2468\n",
            "Doc: 0, Word: 'dwindling', TF-IDF: 0.4572\n",
            "Doc: 0, Word: 'band', TF-IDF: 0.3643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "top_k = 5 #displaying the top 5 words per document\n",
        "top_words_per_doc = defaultdict(list) #initializing a dictionary for this key value pairs\n",
        "\n",
        "for (doc_id, word), score in tfidf.items(): #organizing these values by document id\n",
        "    top_words_per_doc[doc_id].append((word, score))\n",
        "\n",
        "for doc_id in sorted(top_words_per_doc.keys())[:5]:  #looping through these first five documents\n",
        "    sorted_words = sorted(top_words_per_doc[doc_id], key=lambda x: x[1], reverse=True) #sort in descending order\n",
        "    print(f\"\\nTop {top_k} words for document {doc_id}:\") #printing the top 5 words for this document by tf-idf score\n",
        "    for word, score in sorted_words[:top_k]:\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "tfidf_rows = [{\"id\": doc_id, \"word\": word, \"tfidf\": score} for (doc_id, word), score in tfidf.items()] #converting to a list of dictionaries\n",
        "tfidf_df = spark.createDataFrame(tfidf_rows) #creating a PySpark dataframe from the list of dictionaries\n",
        "tfidf_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQKuCbFT1xTa",
        "outputId": "b6128eae-db65-4b3d-8ac2-2a672e059e15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 words for document 0:\n",
            "  cynics: 0.5637\n",
            "  wall: 0.5116\n",
            "  claw: 0.4991\n",
            "  dwindling: 0.4572\n",
            "  sellers: 0.4468\n",
            "\n",
            "Top 5 words for document 1:\n",
            "  carlyle: 0.7168\n",
            "  occasionally: 0.3327\n",
            "  timed: 0.3245\n",
            "  bets: 0.2786\n",
            "  aerospace: 0.2581\n",
            "\n",
            "Top 5 words for document 2:\n",
            "  outlook: 0.4265\n",
            "  doldrums: 0.3770\n",
            "  economy: 0.3721\n",
            "  depth: 0.3134\n",
            "  hang: 0.3048\n",
            "\n",
            "Top 5 words for document 3:\n",
            "  pipeline: 0.4721\n",
            "  main: 0.3649\n",
            "  oil: 0.3576\n",
            "  southern: 0.3366\n",
            "  flows: 0.2774\n",
            "\n",
            "Top 5 words for document 4:\n",
            "  menace: 0.5747\n",
            "  tearaway: 0.3919\n",
            "  straining: 0.2904\n",
            "  toppling: 0.2796\n",
            "  wallets: 0.2665\n",
            "+---+-------------------+----------+\n",
            "| id|              tfidf|      word|\n",
            "+---+-------------------+----------+\n",
            "|  0| 0.5115985326511431|      wall|\n",
            "|  0| 0.2584728642725166|        st|\n",
            "|  0| 0.3372044607529448|     bears|\n",
            "|  0|  0.499114829314058|      claw|\n",
            "|  0| 0.1892216338539946|      back|\n",
            "|  0| 0.2953171727366614|     black|\n",
            "|  0|0.24754017186645658|   reuters|\n",
            "|  0| 0.2773120373951269|     short|\n",
            "|  0| 0.4468379768438066|   sellers|\n",
            "|  0|0.24678348986493034|    street|\n",
            "|  0| 0.4572386180709258| dwindling|\n",
            "|  0| 0.3643421454792778|      band|\n",
            "|  0| 0.4125512394225831|     ultra|\n",
            "|  0|  0.563734318747707|    cynics|\n",
            "|  0|0.37743394553516213|    seeing|\n",
            "|  0| 0.2877107940095433|     green|\n",
            "|  1| 0.7168306746824437|   carlyle|\n",
            "|  1| 0.1973537176743789|     looks|\n",
            "|  1| 0.1898997183872362|    toward|\n",
            "|  1| 0.2057832028092643|commercial|\n",
            "+---+-------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: SVM Objective Function"
      ],
      "metadata": {
        "id": "Hh0z21ORjN-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMwt37bdjMhQ",
        "outputId": "26a678a7-8d71-44f4-8d49-92cbad9dbe6d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   6789      0 --:--:-- --:--:-- --:--:--  6818\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0    112      0 --:--:-- --:--:-- --:--:--   112\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  61.6M      0  0:00:01  0:00:01 --:--:-- 61.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SVM Loss\").getOrCreate()\n",
        "data_df = spark.read.csv(\"data_for_svm.csv\", inferSchema=True, header=False) #reading in the dataset without the header\n",
        "\n",
        "Xy_df = spark.read.csv(\"data_for_svm.csv\", inferSchema=True, header=False)\n",
        "X = Xy_df.rdd.map(lambda row: [float(row[i]) for i in range(64)]) #extracting features X as an rdd of lists of length 64\n",
        "y = Xy_df.rdd.map(lambda row: float(row[64])) #extracting labels Y as an rdd of floating point values\n",
        "\n",
        "#loading weights and biases using pandas\n",
        "w = pd.read_csv(\"w.csv\", header=None).values.flatten()\n",
        "b = pd.read_csv(\"bias.csv\", header=None).iloc[0, 0]\n"
      ],
      "metadata": {
        "id": "uIFLaxRzjUV6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_phase_svm(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Map function that calculate hinge loss for each data point\n",
        "    Takes vectors, labels, weights, and bias as inputs and returns an RDD of hinge loss values\n",
        "    \"\"\"\n",
        "    #creating a zip that returns tuples that turns features and labels into an rdd of x and y pairs\n",
        "    #calculating hinge loss\n",
        "    return X.zip(y).map(lambda xy: max(0, 1 - xy[1] * (np.dot(w, xy[0]) + b)))\n"
      ],
      "metadata": {
        "id": "EEc8-bLjuGog"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_phase_svm(hinge_losses, w, lambd, n):\n",
        "    \"\"\"\n",
        "    Reduce function that sums hinge losses and adds regularization\n",
        "    Takes hinge loss values, weights, regularization parameter, and total number of data points as inputs and returns the total loss\n",
        "    \"\"\"\n",
        "    hinge_sum = hinge_losses.reduce(lambda a, b: a + b) #summing all hinge loss values using rdd reduction\n",
        "    reg_term = lambd * np.dot(w, w) #calculating the regularization term\n",
        "    return reg_term + (hinge_sum / n) #finding total loss\n"
      ],
      "metadata": {
        "id": "wglWFkJmuHt5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_SVM(w, b, X, y, lambd=0.01): #taking 0.01 as the regularization parameter to avoid overfitting\n",
        "    \"\"\"\n",
        "    Calculates the SVM objective using explicit MapReduce phases\n",
        "    Takes weights, bias, feature matrix, labels, and regularization parameter as inputs and returns the total loss calculated as:\n",
        "        L(w, b) = λ * ||w||² + (1/n) * Σ hinge_lossᵢ\n",
        "    \"\"\"\n",
        "    n = X.count() #counting the number of training examples\n",
        "    hinge_losses = map_phase_svm(X, y, w, b) #computing hinge loss for each example\n",
        "    return reduce_phase_svm(hinge_losses, w, lambd, n) #summing the hinge loss and adding regularization\n"
      ],
      "metadata": {
        "id": "-rhMa8oeuLHr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_val = loss_SVM(w, b, X, y) #calling the function\n",
        "print(f\"SVM Loss: {loss_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jxOO8n7jpFp",
        "outputId": "75b9c02d-d5cd-4f78-f44d-7d07ce78d826"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Loss: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_SVM(w, b, X):\n",
        "    \"\"\"\n",
        "    MapReduce SVM prediction function\n",
        "    Takes weights, biases, and rdd of vectors as inputs and returns rdd of predicted values\n",
        "    \"\"\"\n",
        "    return X.map(lambda x: 1 if np.dot(w, x) + b >= 0 else -1) #for each x computing the sign using (wᵗx + b)\n"
      ],
      "metadata": {
        "id": "YY4wwpUwtsjx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_SVM(w, b, X) #calling the predict function\n",
        "print(predictions.take(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DCxxRcGtvrz",
        "outputId": "f598bef8-c2ea-4ded-d8ff-32e34809ad6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1, -1, -1, 1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI Disclosure:\n",
        "(1) I used GenAI for help with syntax for pyspark and rdd operations, and in depth help specifically for the map and reduce functions of part 1.\n",
        "(2) ChatGPT 4o.\n",
        "(3) The prompts I used to get the results are as follows:\n",
        "What are the different ways in which I can write MapReduce functions to calculate tf-idf? I am looking to follow the structure of breaking down the functions and writing them in map, reduce, shuffle and sort steps.\n",
        "What is the cleanest way to calculate hinge loss in the SVM function using the same breakdown of functions?\n"
      ],
      "metadata": {
        "id": "U1XOcwWQ5T2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "readme_content = \"\"\"\n",
        "# Homework 3 – MapReduce with PySpark\n",
        "\n",
        "## Overview\n",
        "This project includes:\n",
        "- **Part 1**: TF-IDF computation\n",
        "- **Part 2**: SVM soft-margin loss and predictions\n",
        "\n",
        "## How to Run (inside Docker)\n",
        "\n",
        "### 1. Build the container:\n",
        "```bash\n",
        "docker build -t homework3 .\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "a53pQmg09a72"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}