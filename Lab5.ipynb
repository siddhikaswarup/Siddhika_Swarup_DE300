{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff781e6-0c0f-45da-a6ac-cbdc74b3765b",
   "metadata": {},
   "source": [
    "## High-Level ETL (Extract - Transform - Load) Flow\n",
    "**Goal**: By the end of this tutorial, you will be able to\n",
    "- Extract: Download a file from AWS S3 using Pythonâ€™s boto3.\n",
    "- Transform: Clean, filter, or manipulate data in Python (often using libraries like pandas).\n",
    "- Load: Insert the transformed data into a relational database via SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4c364-5538-4205-bc71-4a9e631eeefc",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "\n",
    "1. Implement the following functions\n",
    "   - `extract_from_csv(file_to_process: str) -> pd.DataFrame`: read the .csv file and return dataframe\n",
    "   - `extract_from_json(file_to_process: str) -> pd.DataFrame`: read the .json file and return dataframe\n",
    "   - `extract() -> pd.DataFrame`: extract data of heterogeneous format and combine them into a single dataframe.\n",
    "   - `transform(df) -> pd.DataFrame`: function for data cleaning and manipulation.\n",
    "2. Clean the data\n",
    "   - Round float-type columns to two decimal places.\n",
    "   - remove duplicate samples\n",
    "   - Save the cleaned data into parquet file\n",
    "3. Insert the data into SQL\n",
    "   - Create postgresql database\n",
    "   - Insert the data into the database\n",
    "  \n",
    "Submission requirement:\n",
    "    1. Jupyter Notebook\n",
    "    2. Parquet File\n",
    "    3. SQL file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "caaaa4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.40)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (19.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\siddh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\siddh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\siddh\\appdata\\roaming\\python\\python311\\site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siddh\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas psycopg2 sqlalchemy pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9594048-5e1f-4138-b202-30eabce92d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Package:\n",
    "# psycopg2 2.9.10 (A PostgreSQL database adapter)\n",
    "# pandas 2.0.3 (For data manipulation and analysis)\n",
    "# sqlalchemy 2.0.37 (A SQL toolkit and Object Relational Mapper)\n",
    "# pyarrow 14.0.1 (Provides support for efficient in-memory columnar data structures, part from Apache Arrow Objective)\n",
    "import pandas as pd\n",
    "\n",
    "#required for reading .xml files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#required for navigating machine's directory\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "#required for communicating with SQL database\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ea5ad-4aa4-4d75-8cca-41de1e7454f3",
   "metadata": {},
   "source": [
    "# E: Extracting data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964aa80-e1d5-42bf-8159-6ad91273a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "my_aws_access_key_id='...'\n",
    "my_aws_secret_access_key='...'\n",
    "my_aws_session_token='...'\n",
    "\n",
    "BUCKET_NAME = 'de300spring2025'   # Replace with your bucket name\n",
    "S3_FOLDER = 'dinglin_xia/lab4_data/'             # The folder path in S3\n",
    "LOCAL_DIR = './local-data/'      # Local directory to save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3161fa0-3999-4aff-a1e8-e0f364fd5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_folder(bucket_name, s3_folder, local_dir):\n",
    "    \"\"\"Download a folder from S3.\"\"\"\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    # List objects within the specified folder\n",
    "    s3_resource = boto3.resource('s3',\n",
    "                                aws_access_key_id=my_aws_access_key_id,\n",
    "                                aws_secret_access_key=my_aws_secret_access_key,\n",
    "                                aws_session_token=my_aws_session_token)\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        # Define local file path\n",
    "        local_file_path = os.path.join(local_dir, obj.key[len(s3_folder):])  \n",
    "        \n",
    "        if obj.key.endswith('/'):  # Skip folders\n",
    "            continue\n",
    "        \n",
    "        # Create local directory if needed\n",
    "        local_file_dir = os.path.dirname(local_file_path)\n",
    "        if not os.path.exists(local_file_dir):\n",
    "            os.makedirs(local_file_dir)\n",
    "        \n",
    "        # Download the file\n",
    "        bucket.download_file(obj.key, local_file_path)\n",
    "        print(f\"Downloaded {obj.key} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca57a83a-ceb6-4cb8-9101-75426a5728f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.csv to ./local-data/used_car_prices1.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.json to ./local-data/used_car_prices1.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.xml to ./local-data/used_car_prices1.xml\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.csv to ./local-data/used_car_prices2.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.json to ./local-data/used_car_prices2.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.xml to ./local-data/used_car_prices2.xml\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.csv to ./local-data/used_car_prices3.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.json to ./local-data/used_car_prices3.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.xml to ./local-data/used_car_prices3.xml\n"
     ]
    }
   ],
   "source": [
    "download_s3_folder(BUCKET_NAME, S3_FOLDER, LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14027a8-8a6d-4566-93ba-710e68b97f3e",
   "metadata": {},
   "source": [
    "## Extract data from ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03b2bdcb-fb78-419c-a3e9-9f12ee8f2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./local-data\\used_car_prices1.csv\n",
      "./local-data\\used_car_prices1.json\n",
      "./local-data\\used_car_prices1.xml\n",
      "./local-data\\used_car_prices2.csv\n",
      "./local-data\\used_car_prices2.json\n",
      "./local-data\\used_car_prices2.xml\n",
      "./local-data\\used_car_prices3.csv\n",
      "./local-data\\used_car_prices3.json\n",
      "./local-data\\used_car_prices3.xml\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('./local-data/*')\n",
    "\n",
    "# Output the list of files\n",
    "for file in all_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30b5a8-7206-4a8c-8837-b42ab76115fb",
   "metadata": {},
   "source": [
    "### Function to extract data from one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ac06a7b-ea5d-4410-b0b3-be9db100d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_to_process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051e6b2-6f3a-4b2c-9ff4-1e10a91a296e",
   "metadata": {},
   "source": [
    "### Function to extract data from one .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31b19053-3d8a-4386-82d9-5a7b287d37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(file_to_process: str) -> pd.DataFrame:\n",
    "    return pd.read_json(file_to_process, lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072dfa48-6805-451e-9d0f-eb09de70e9b1",
   "metadata": {},
   "source": [
    "### Function to extract data from one  .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f6bd897-d626-43f3-9980-e3c1e0e27ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_xml(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.DataFrame(columns = columns)\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        car_model = person.find(\"car_model\").text\n",
    "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
    "        price = float(person.find(\"price\").text)\n",
    "        fuel = person.find(\"fuel\").text\n",
    "        sample = pd.DataFrame({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, index = [0])\n",
    "        dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2e693-0d74-4e9e-92e1-395119aab10e",
   "metadata": {},
   "source": [
    "### Function to extract data from the ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fbc266d-1ef2-49bb-8784-4a8ccbffd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract() -> pd.DataFrame:\n",
    "    extracted_data = pd.DataFrame(columns = columns)\n",
    "    #for csv files\n",
    "    for csv_file in glob.glob(os.path.join(folder, \"*.csv\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
    "\n",
    "    # For JSON files\n",
    "    for json_file in glob.glob(os.path.join(folder, \"*.json\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_json(json_file)], ignore_index=True)\n",
    "\n",
    "    # For XML files\n",
    "    for xml_file in glob.glob(os.path.join(folder, \"*.xml\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_xml(xml_file)], ignore_index=True)\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cafa18-245e-4e2c-a63b-6854eb93b863",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a70a8ce-348f-4b26-8a9a-3ead42565c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_16560\\2449669802.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_16560\\3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_16560\\3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_16560\\3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "columns = ['car_model','year_of_manufacture','price', 'fuel']\n",
    "folder = \"local-data\"\n",
    "#table_name = \"car_data\"\n",
    "\n",
    "# run\n",
    "def main():\n",
    "    data = extract()\n",
    "    #insert_to_table(data, \"car_data\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b1288a8-86fb-498c-833a-682ecd9c1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.552239</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.895522</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.731343</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.671642</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model year_of_manufacture         price    fuel\n",
       "0      ritz                2014   5000.000000  Petrol\n",
       "1       sx4                2013   7089.552239  Diesel\n",
       "2      ciaz                2017  10820.895522  Petrol\n",
       "3   wagon r                2011   4253.731343  Petrol\n",
       "4     swift                2014   6865.671642  Diesel"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265c408-8c63-413f-ad04-8d391e5b564d",
   "metadata": {},
   "source": [
    "# T: Transformation data and save organized data to .parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faae9e13-ded5-47e0-8316-13ec6084ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_file = \"cars.parquet\"\n",
    "staging_data_dir = \"staging_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eefdc72e-6359-4363-a648-86082888f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # truncate price with 2 decimal place (add your code below)\n",
    "    df['price'] = df['price'].astype(float).round(2)\n",
    "    # remove samples with same car_model (add your code below)\n",
    "    df = df.drop_duplicates(subset='car_model', keep='first')\n",
    "\n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # Ensure the staging directory exists before writing the Parquet file\n",
    "    if not os.path.exists(staging_data_dir):\n",
    "        os.makedirs(staging_data_dir)\n",
    "        print(f\"Directory '{staging_data_dir}' created.\")\n",
    "\n",
    "    # write to parquet\n",
    "    df.to_parquet(os.path.join(staging_data_dir, staging_file))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f5d5423-c198-4aa7-89f7-354992f97324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (90, 4)\n",
      "Shape of data (25, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.55</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.90</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.73</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.67</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model year_of_manufacture     price    fuel\n",
       "0      ritz                2014   5000.00  Petrol\n",
       "1       sx4                2013   7089.55  Diesel\n",
       "2      ciaz                2017  10820.90  Petrol\n",
       "3   wagon r                2011   4253.73  Petrol\n",
       "4     swift                2014   6865.67  Diesel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the head of your data\n",
    "df = transform(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960d531-a731-42d4-906c-4602391a16ca",
   "metadata": {},
   "source": [
    "# L: Loading data for further modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01f8d-a8b6-454c-a842-61c44cf04efc",
   "metadata": {},
   "source": [
    "### Set Up PostgreSQL Locally\n",
    "#### Step 1: Install PostgreSQL\n",
    "- Windows: Download from MySQL Official Site {https://www.postgresql.org/download/}\n",
    "- Mac:\n",
    "  ```{bash}\n",
    "  brew install postgresql\n",
    "  brew services start postgresql\n",
    "  ```\n",
    "Then access PostgreSQL CLI\n",
    "```{bash}\n",
    "psql -U postgres\n",
    "```\n",
    "Note: if you don't have default \"postgres\" user, then create it manually by\n",
    "```{bash}\n",
    "default \"postgres\" user\n",
    "```\n",
    "or\n",
    "```{bash}\n",
    "sudo -u $(whoami) createuser postgres -s\n",
    "```\n",
    "\n",
    "Then create a database\n",
    "```{sql}\n",
    "CREATE DATABASE my_local_db;\n",
    "\\l  -- List all databases\n",
    "```\n",
    "\n",
    "#### Step 2: Create a User and Grant Privileges\n",
    "In PostgreSQL CLI:\n",
    "```{sql}\n",
    "CREATE USER myuser WITH ENCRYPTED PASSWORD 'mypassword';\n",
    "GRANT ALL PRIVILEGES ON DATABASE my_local_db TO myuser;\n",
    "```\n",
    "\n",
    "#### Step 3: Install Required Python Libraries\n",
    "```{bash}\n",
    "pip install pandas sqlalchemy pymysql psycopg2 mysql-connector-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270a29e-c73a-4ca1-b7e8-64ec621cb29a",
   "metadata": {},
   "source": [
    "### Utility function for writing data into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f188de2-ae27-444c-8590-4c1dde9ac7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_host = \"localhost\"\n",
    "db_user = \"your_user_name\"\n",
    "db_password = \"your_password\"\n",
    "db_name = \"your_database_name\"\n",
    "\n",
    "conn_string = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb749a-4467-4264-b23b-f0693ee980af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "df = pd.read_sql(\"SELECT * FROM pg_catalog.pg_tables;\", con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fed596-5b37-4da6-927d-facc292e733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_table(data: pd.DataFrame, conn_string:str, table_name:str):\n",
    "    db = create_engine(conn_string) # creates a connection to the database using SQLAlchemy\n",
    "    conn = db.connect() # Establishes a database connection\n",
    "    data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20ac0d-c828-45a1-b29a-a8cf20e5cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the .parquet file\n",
    "\n",
    "def load() -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    for parquet_file in glob.glob(os.path.join(staging_data_dir, \"*.parquet\")):\n",
    "        data = pd.concat([pd.read_parquet(parquet_file),data])\n",
    "\n",
    "    #insert_to_table(data, table_name)\n",
    "    insert_to_table(data = data, conn_string = conn_string, table_name = 'ml_car_data')\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load()\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
